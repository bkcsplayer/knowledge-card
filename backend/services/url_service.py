"""
URL Content Fetching Service
æŠ“å– URL å†…å®¹ï¼Œç‰¹åˆ«ä¼˜åŒ– GitHub é¡¹ç›®å¤„ç†

æ”¯æŒï¼š
1. GitHub ä»“åº“ - ä½¿ç”¨ GitHub API è·å–å®Œæ•´ä¿¡æ¯
2. æ™®é€šç½‘é¡µ - æŠ“å– HTML å¹¶æå–ä¸»è¦å†…å®¹
3. å…¶ä»– URL - åŸºç¡€å¤„ç†
"""

import re
import logging
import aiohttp
from typing import Dict, Any, Optional, List
from urllib.parse import urlparse

logger = logging.getLogger(__name__)


class URLService:
    """URL å†…å®¹æŠ“å–æœåŠ¡"""
    
    def __init__(self):
        self.github_api = "https://api.github.com"
        self.timeout = aiohttp.ClientTimeout(total=30)
        self.headers = {
            "User-Agent": "Knowledge-Distillery/1.0",
            "Accept": "application/json"
        }
    
    def is_github_url(self, url: str) -> bool:
        """æ£€æµ‹æ˜¯å¦æ˜¯ GitHub URL"""
        return "github.com" in url.lower()
    
    def parse_github_url(self, url: str) -> Optional[Dict[str, str]]:
        """
        è§£æ GitHub URLï¼Œæå– owner å’Œ repo
        
        æ”¯æŒæ ¼å¼ï¼š
        - https://github.com/owner/repo
        - https://github.com/owner/repo/xxx
        - github.com/owner/repo
        """
        patterns = [
            r'github\.com[/:]([^/]+)/([^/\s?#]+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                owner = match.group(1)
                repo = match.group(2).replace('.git', '')
                return {"owner": owner, "repo": repo}
        
        return None
    
    async def fetch_github_repo(self, owner: str, repo: str) -> Dict[str, Any]:
        """
        é€šè¿‡ GitHub API è·å–ä»“åº“è¯¦ç»†ä¿¡æ¯
        """
        result = {
            "type": "github_repo",
            "owner": owner,
            "repo": repo,
            "url": f"https://github.com/{owner}/{repo}",
            "success": False
        }
        
        try:
            async with aiohttp.ClientSession(timeout=self.timeout) as session:
                # è·å–ä»“åº“åŸºæœ¬ä¿¡æ¯
                repo_url = f"{self.github_api}/repos/{owner}/{repo}"
                async with session.get(repo_url, headers=self.headers) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        result.update({
                            "success": True,
                            "name": data.get("name"),
                            "full_name": data.get("full_name"),
                            "description": data.get("description") or "æ— æè¿°",
                            "homepage": data.get("homepage"),
                            "stars": data.get("stargazers_count", 0),
                            "forks": data.get("forks_count", 0),
                            "watchers": data.get("watchers_count", 0),
                            "open_issues": data.get("open_issues_count", 0),
                            "language": data.get("language"),
                            "topics": data.get("topics", []),
                            "license": data.get("license", {}).get("name") if data.get("license") else None,
                            "default_branch": data.get("default_branch", "main"),
                            "created_at": data.get("created_at"),
                            "updated_at": data.get("updated_at"),
                            "pushed_at": data.get("pushed_at"),
                            "is_fork": data.get("fork", False),
                            "is_archived": data.get("archived", False),
                            "html_url": data.get("html_url"),
                            "clone_url": data.get("clone_url"),
                            "size_kb": data.get("size", 0)
                        })
                    else:
                        logger.warning(f"GitHub API returned {resp.status} for {owner}/{repo}")
                        result["error"] = f"GitHub API è¿”å› {resp.status}"
                
                # è·å– README å†…å®¹
                if result["success"]:
                    readme_url = f"{self.github_api}/repos/{owner}/{repo}/readme"
                    async with session.get(readme_url, headers={**self.headers, "Accept": "application/vnd.github.raw"}) as resp:
                        if resp.status == 200:
                            readme_content = await resp.text()
                            # é™åˆ¶ README é•¿åº¦
                            result["readme"] = readme_content[:8000] if len(readme_content) > 8000 else readme_content
                            result["readme_truncated"] = len(readme_content) > 8000
                        else:
                            result["readme"] = None
                
                # è·å–è¯­è¨€ç»Ÿè®¡
                if result["success"]:
                    langs_url = f"{self.github_api}/repos/{owner}/{repo}/languages"
                    async with session.get(langs_url, headers=self.headers) as resp:
                        if resp.status == 200:
                            result["languages"] = await resp.json()
                
                # è·å–æœ€è¿‘ releases
                if result["success"]:
                    releases_url = f"{self.github_api}/repos/{owner}/{repo}/releases?per_page=3"
                    async with session.get(releases_url, headers=self.headers) as resp:
                        if resp.status == 200:
                            releases = await resp.json()
                            result["releases"] = [
                                {
                                    "tag": r.get("tag_name"),
                                    "name": r.get("name"),
                                    "published_at": r.get("published_at"),
                                    "prerelease": r.get("prerelease")
                                }
                                for r in releases[:3]
                            ]
                
        except aiohttp.ClientError as e:
            logger.error(f"GitHub API request failed: {e}")
            result["error"] = f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"
        except Exception as e:
            logger.error(f"GitHub fetch error: {e}")
            result["error"] = f"å¤„ç†å¤±è´¥: {str(e)}"
        
        return result
    
    async def fetch_webpage(self, url: str) -> Dict[str, Any]:
        """
        æŠ“å–æ™®é€šç½‘é¡µå†…å®¹
        """
        result = {
            "type": "webpage",
            "url": url,
            "success": False
        }
        
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Accept": "text/html,application/xhtml+xml"
            }
            
            async with aiohttp.ClientSession(timeout=self.timeout) as session:
                async with session.get(url, headers=headers, allow_redirects=True) as resp:
                    if resp.status == 200:
                        html = await resp.text()
                        
                        # æå–æ ‡é¢˜
                        title_match = re.search(r'<title[^>]*>([^<]+)</title>', html, re.IGNORECASE)
                        title = title_match.group(1).strip() if title_match else None
                        
                        # æå– meta description
                        desc_match = re.search(
                            r'<meta[^>]*name=["\']description["\'][^>]*content=["\']([^"\']+)["\']',
                            html, re.IGNORECASE
                        )
                        description = desc_match.group(1).strip() if desc_match else None
                        
                        # æå–æ­£æ–‡ï¼ˆç®€å•å¤„ç†ï¼šç§»é™¤è„šæœ¬å’Œæ ·å¼ï¼Œæå–æ–‡æœ¬ï¼‰
                        # ç§»é™¤ script å’Œ style
                        clean = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
                        clean = re.sub(r'<style[^>]*>.*?</style>', '', clean, flags=re.DOTALL | re.IGNORECASE)
                        # ç§»é™¤ HTML æ ‡ç­¾
                        clean = re.sub(r'<[^>]+>', ' ', clean)
                        # æ¸…ç†ç©ºç™½
                        clean = re.sub(r'\s+', ' ', clean).strip()
                        
                        # æˆªå–æ­£æ–‡
                        body_text = clean[:5000] if len(clean) > 5000 else clean
                        
                        result.update({
                            "success": True,
                            "title": title,
                            "description": description,
                            "content": body_text,
                            "content_length": len(clean),
                            "truncated": len(clean) > 5000
                        })
                    else:
                        result["error"] = f"HTTP {resp.status}"
                        
        except aiohttp.ClientError as e:
            logger.error(f"Webpage fetch failed: {e}")
            result["error"] = f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"
        except Exception as e:
            logger.error(f"Webpage fetch error: {e}")
            result["error"] = f"å¤„ç†å¤±è´¥: {str(e)}"
        
        return result
    
    async def fetch_url(self, url: str) -> Dict[str, Any]:
        """
        è‡ªåŠ¨æ£€æµ‹ URL ç±»å‹å¹¶è·å–å†…å®¹
        
        è¿”å›æ ¼å¼ï¼š
        {
            "type": "github_repo" | "webpage" | "unknown",
            "url": "åŸå§‹ URL",
            "success": bool,
            "error": "é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœ‰ï¼‰",
            ... å…¶ä»–å­—æ®µå–å†³äºç±»å‹
        }
        """
        if not url or not url.strip():
            return {"type": "unknown", "url": "", "success": False, "error": "URL ä¸ºç©º"}
        
        url = url.strip()
        
        # ç¡®ä¿æœ‰åè®®
        if not url.startswith(("http://", "https://")):
            url = "https://" + url
        
        # GitHub ä»“åº“
        if self.is_github_url(url):
            parsed = self.parse_github_url(url)
            if parsed:
                logger.info(f"Fetching GitHub repo: {parsed['owner']}/{parsed['repo']}")
                return await self.fetch_github_repo(parsed["owner"], parsed["repo"])
        
        # æ™®é€šç½‘é¡µ
        logger.info(f"Fetching webpage: {url}")
        return await self.fetch_webpage(url)
    
    def format_github_for_distillation(self, data: Dict[str, Any]) -> str:
        """
        å°† GitHub æ•°æ®æ ¼å¼åŒ–ä¸ºä¾› AI åˆ†æçš„æ–‡æœ¬
        """
        if not data.get("success"):
            return f"æ— æ³•è·å– GitHub ä»“åº“ä¿¡æ¯: {data.get('error', 'æœªçŸ¥é”™è¯¯')}"
        
        lines = [
            f"# GitHub é¡¹ç›®: {data.get('full_name')}",
            "",
            f"**ä»“åº“åœ°å€**: {data.get('html_url')}",
            f"**æè¿°**: {data.get('description')}",
            "",
            "## é¡¹ç›®ç»Ÿè®¡",
            f"- â­ Stars: {data.get('stars', 0):,}",
            f"- ğŸ”± Forks: {data.get('forks', 0):,}",
            f"- ğŸ‘ï¸ Watchers: {data.get('watchers', 0):,}",
            f"- ğŸ“‹ Issues: {data.get('open_issues', 0)}",
            f"- ğŸ’¾ å¤§å°: {data.get('size_kb', 0):,} KB",
            "",
            f"**ä¸»è¦è¯­è¨€**: {data.get('language') or 'æœªçŸ¥'}",
            f"**è®¸å¯è¯**: {data.get('license') or 'æœªæŒ‡å®š'}",
            f"**åˆ›å»ºæ—¶é—´**: {data.get('created_at', '')[:10] if data.get('created_at') else 'æœªçŸ¥'}",
            f"**æœ€åæ›´æ–°**: {data.get('pushed_at', '')[:10] if data.get('pushed_at') else 'æœªçŸ¥'}",
        ]
        
        # Topics
        if data.get("topics"):
            lines.append("")
            lines.append(f"**æ ‡ç­¾**: {', '.join(data['topics'])}")
        
        # è¯­è¨€ç»Ÿè®¡
        if data.get("languages"):
            lines.append("")
            lines.append("## è¯­è¨€å æ¯”")
            total = sum(data["languages"].values())
            for lang, bytes_count in sorted(data["languages"].items(), key=lambda x: -x[1])[:5]:
                pct = (bytes_count / total * 100) if total > 0 else 0
                lines.append(f"- {lang}: {pct:.1f}%")
        
        # Releases
        if data.get("releases"):
            lines.append("")
            lines.append("## æœ€æ–°ç‰ˆæœ¬")
            for rel in data["releases"]:
                tag = rel.get("tag") or "æ— æ ‡ç­¾"
                name = rel.get("name") or tag
                date = rel.get("published_at", "")[:10] if rel.get("published_at") else ""
                lines.append(f"- {name} ({date})")
        
        # Homepage
        if data.get("homepage"):
            lines.append("")
            lines.append(f"**å®˜æ–¹ç½‘ç«™**: {data['homepage']}")
        
        # Clone URL
        lines.append("")
        lines.append("## å®‰è£…ä½¿ç”¨")
        lines.append(f"```bash")
        lines.append(f"git clone {data.get('clone_url')}")
        lines.append(f"cd {data.get('repo')}")
        lines.append(f"```")
        
        # README æ‘˜è¦
        if data.get("readme"):
            lines.append("")
            lines.append("## README å†…å®¹")
            readme_preview = data["readme"][:3000]
            if len(data["readme"]) > 3000:
                readme_preview += "\n\n[... README å†…å®¹å·²æˆªæ–­ ...]"
            lines.append(readme_preview)
        
        return "\n".join(lines)
    
    def format_webpage_for_distillation(self, data: Dict[str, Any]) -> str:
        """
        å°†ç½‘é¡µæ•°æ®æ ¼å¼åŒ–ä¸ºä¾› AI åˆ†æçš„æ–‡æœ¬
        """
        if not data.get("success"):
            return f"æ— æ³•è·å–ç½‘é¡µå†…å®¹: {data.get('error', 'æœªçŸ¥é”™è¯¯')}"
        
        lines = [
            f"# ç½‘é¡µ: {data.get('title') or 'æ— æ ‡é¢˜'}",
            "",
            f"**URL**: {data.get('url')}",
        ]
        
        if data.get("description"):
            lines.append(f"**æè¿°**: {data['description']}")
        
        lines.append("")
        lines.append("## é¡µé¢å†…å®¹")
        lines.append(data.get("content", "æ— å†…å®¹"))
        
        return "\n".join(lines)


# å…¨å±€å®ä¾‹
url_service = URLService()

