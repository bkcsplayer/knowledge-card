"""
Knowledge Distillation Pipeline
å¤šé˜¶æ®µã€å¤šæ¨¡å‹æ·±åº¦çŸ¥è¯†è’¸é¦ç³»ç»Ÿ

æµç¨‹ï¼š
1. æå–é˜¶æ®µ (Extract) - ä»åŸå§‹å†…å®¹/å›¾ç‰‡æå–åŸºç¡€ä¿¡æ¯
2. åˆ†æé˜¶æ®µ (Analyze) - æ·±åº¦ç†è§£å†…å®¹ç»“æ„å’ŒæŠ€æœ¯ç»†èŠ‚
3. æœç´¢é˜¶æ®µ (Search) - æœç´¢ç›¸å…³ä¿¡æ¯è¡¥å……ä¸Šä¸‹æ–‡
4. éªŒè¯é˜¶æ®µ (Verify) - äº¤å‰éªŒè¯å…³é”®ä¿¡æ¯
5. å½’çº³é˜¶æ®µ (Synthesize) - ç»¼åˆæ‰€æœ‰ä¿¡æ¯ç”ŸæˆçŸ¥è¯†å¡ç‰‡
"""

import json
import re
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime

from services.ai_service import ai_service
from services.telegram_service import get_telegram_service
from config import settings

logger = logging.getLogger(__name__)


class DistillationPipeline:
    """å¤šé˜¶æ®µçŸ¥è¯†è’¸é¦ç®¡é“"""
    
    def __init__(self):
        self.stages = [
            "extract",      # æå–
            "analyze",      # åˆ†æ
            "search",       # æœç´¢
            "verify",       # éªŒè¯
            "synthesize"    # å½’çº³
        ]
    
    async def _notify(self, message: str):
        """å‘é€ Telegram é€šçŸ¥"""
        try:
            service = get_telegram_service()
            if service and service.enabled and service.chat_id:
                await service.send_message(message)
        except Exception as e:
            logger.warning(f"Telegram notification failed: {e}")
    
    async def run(
        self, 
        content: str = "", 
        images: Optional[List[str]] = None,
        knowledge_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„è’¸é¦ç®¡é“ï¼ˆå¸¦å›é€€æœºåˆ¶ï¼‰
        
        Args:
            content: åŸå§‹æ–‡æœ¬å†…å®¹
            images: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            knowledge_id: çŸ¥è¯†æ¡ç›®IDï¼ˆç”¨äºé€šçŸ¥ï¼‰
        
        Returns:
            å®Œæ•´çš„çŸ¥è¯†å¡ç‰‡æ•°æ®
        """
        kid = knowledge_id or 0
        
        # é¦–å…ˆå°è¯•ç®€åŒ–ç‰ˆçš„å•æ¬¡è’¸é¦ï¼ˆæ›´å¯é ï¼‰
        await self._notify(f"ğŸ§ª #{kid} | å¼€å§‹ AI çŸ¥è¯†è’¸é¦...")
        
        try:
            result = await self._simple_distill(content, images, kid)
            if result and not result.get("error"):
                await self._notify(f"ğŸ‰ #{kid} | è’¸é¦å®Œæˆ!\nğŸ“ {result.get('title', '')[:50]}\nğŸ·ï¸ {', '.join(result.get('tags', [])[:5])}")
                return result
        except Exception as e:
            logger.warning(f"Simple distill failed, trying pipeline: {e}")
        
        # å¦‚æœç®€åŒ–ç‰ˆå¤±è´¥ï¼Œå°è¯•å¤šé˜¶æ®µç®¡é“
        await self._notify(f"âš™ï¸ #{kid} | å¯ç”¨å¤šé˜¶æ®µæ·±åº¦åˆ†æ...")
        
        pipeline_result = {
            "stages_completed": [],
            "raw_extractions": {},
            "final_result": None,
            "errors": []
        }
        
        try:
            # ========== é˜¶æ®µ 1: æå– ==========
            extraction = await self._stage_extract(content, images)
            pipeline_result["raw_extractions"]["extract"] = extraction
            pipeline_result["stages_completed"].append("extract")
            
            if extraction.get("error"):
                raise Exception(f"æå–å¤±è´¥: {extraction.get('error')}")
            
            await self._notify(f"âœ… #{kid} | æå–å®Œæˆ")
            
            # ========== é˜¶æ®µ 2: åˆ†æ ==========
            analysis = await self._stage_analyze(extraction)
            pipeline_result["raw_extractions"]["analyze"] = analysis
            pipeline_result["stages_completed"].append("analyze")
            
            # ========== é˜¶æ®µ 3: æœç´¢è¡¥å…… ==========
            enriched = await self._stage_search(extraction, analysis)
            pipeline_result["raw_extractions"]["search"] = enriched
            pipeline_result["stages_completed"].append("search")
            
            # ========== é˜¶æ®µ 4: éªŒè¯ ==========
            verification = await self._stage_verify(extraction, analysis, enriched)
            pipeline_result["raw_extractions"]["verify"] = verification
            pipeline_result["stages_completed"].append("verify")
            
            confidence = verification.get("confidence", 0.5)
            
            # ========== é˜¶æ®µ 5: å½’çº³æ€»ç»“ ==========
            final = await self._stage_synthesize(extraction, analysis, enriched, verification)
            pipeline_result["final_result"] = final
            pipeline_result["stages_completed"].append("synthesize")
            
            # æ·»åŠ éªŒè¯æ ‡ç­¾
            if confidence >= 0.7:
                tags = final.get("tags", [])
                if "å·²éªŒè¯" not in tags:
                    tags.append("å·²éªŒè¯")
                final["tags"] = tags
            
            await self._notify(f"ğŸ‰ #{kid} | æ·±åº¦åˆ†æå®Œæˆ!\nğŸ“ {final.get('title', '')[:50]}")
            
            return final
            
        except Exception as e:
            error_msg = str(e)
            pipeline_result["errors"].append(error_msg)
            logger.error(f"Pipeline failed: {e}")
            await self._notify(f"âš ï¸ #{kid} | ç®¡é“å¼‚å¸¸ï¼Œä½¿ç”¨åŸºç¡€ç»“æœ")
            
            # è¿”å›æå–é˜¶æ®µçš„åŸºç¡€ç»“æœ
            ext = pipeline_result["raw_extractions"].get("extract", {})
            return {
                "title": ext.get("title", "æœªçŸ¥å†…å®¹"),
                "summary": ext.get("raw_summary", content[:500] if content else "å›¾ç‰‡å†…å®¹"),
                "key_points": ext.get("detected_features", []),
                "tags": ext.get("detected_names", []),
                "category": "æœªåˆ†ç±»",
                "difficulty": "ä¸­çº§",
                "action_items": [],
                "repo_url": (ext.get("detected_urls", []) or [None])[0]
            }
    
    async def _simple_distill(self, content: str, images: Optional[List[str]], kid: int) -> Dict[str, Any]:
        """
        ç®€åŒ–ç‰ˆå•æ¬¡è’¸é¦ - æ›´å¯é 
        """
        # å¤„ç†å›¾ç‰‡
        actual_content = content
        if images and len(images) > 0:
            image_text = await ai_service.analyze_image(
                images, 
                context="è¯·è¯¦ç»†æè¿°å›¾ç‰‡ä¸­çš„æ‰€æœ‰å†…å®¹ã€‚å¦‚æœæ˜¯GitHubé¡µé¢ï¼Œæå–ä»“åº“åã€æè¿°ã€staræ•°ã€æŠ€æœ¯æ ˆç­‰ã€‚å¦‚æœæ˜¯ä»£ç ï¼Œè¯´æ˜ä»£ç åŠŸèƒ½ã€‚"
            )
            if image_text:
                actual_content = f"{image_text}\n\n{content}" if content else image_text
        
        if not actual_content or len(actual_content.strip()) < 10:
            return {"error": "æ²¡æœ‰æœ‰æ•ˆå†…å®¹"}
        
        prompt = """ä½ æ˜¯çŸ¥è¯†ç®¡ç†ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹å†…å®¹å¹¶ç”ŸæˆçŸ¥è¯†å¡ç‰‡ã€‚

è¾“å‡º JSON æ ¼å¼ï¼š
{
    "title": "ç®€æ´çš„æ ‡é¢˜",
    "summary": "150-250å­—çš„å®Œæ•´æ‘˜è¦ï¼ŒåŒ…å«ï¼šæ˜¯ä»€ä¹ˆã€æ ¸å¿ƒåŠŸèƒ½ã€é€‚ç”¨åœºæ™¯",
    "key_points": ["å…³é”®ç‚¹1", "å…³é”®ç‚¹2", "å…³é”®ç‚¹3", "å…³é”®ç‚¹4", "å…³é”®ç‚¹5"],
    "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", "æ ‡ç­¾3"],
    "category": "åˆ†ç±»ï¼ˆæŠ€æœ¯/å·¥å…·/æ•™ç¨‹/æ¦‚å¿µï¼‰",
    "difficulty": "éš¾åº¦ï¼ˆå…¥é—¨/ä¸­çº§/é«˜çº§ï¼‰",
    "action_items": ["å¯æ‰§è¡Œçš„è¡ŒåŠ¨1", "è¡ŒåŠ¨2"],
    "usage_example": "ä½¿ç”¨ç¤ºä¾‹ä»£ç æˆ–å‘½ä»¤ï¼ˆå¦‚é€‚ç”¨ï¼‰",
    "deployment_guide": "éƒ¨ç½²æ­¥éª¤ï¼ˆå¦‚æœæ˜¯é¡¹ç›®ï¼‰",
    "is_open_source": true/false,
    "repo_url": "GitHubåœ°å€ï¼ˆå¦‚æœ‰ï¼‰"
}

è¦æ±‚ï¼š
1. å¦‚æœæ˜¯ GitHub é¡¹ç›®ï¼Œå¿…é¡»æå–ä»“åº“åœ°å€
2. æä¾›å®ç”¨çš„ä½¿ç”¨ç¤ºä¾‹
3. æ ‡ç­¾è¦ç²¾å‡†
4. æ‘˜è¦è¦å…¨é¢"""

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"è¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼š\n\n{actual_content[:4000]}"}
        ]
        
        result = await ai_service._call_api(messages, temperature=0.3)
        
        if not result:
            return {"error": "AI æœªè¿”å›ç»“æœ"}
        
        try:
            return self._parse_json(result)
        except Exception as e:
            logger.error(f"Simple distill parse error: {e}")
            # å°è¯•ä»å“åº”ä¸­æå–æœ‰ç”¨ä¿¡æ¯
            return {
                "title": actual_content[:80],
                "summary": actual_content[:300],
                "key_points": [],
                "tags": [],
                "category": "æœªåˆ†ç±»",
                "difficulty": "ä¸­çº§",
                "action_items": [],
                "error": f"è§£æå¤±è´¥: {str(e)}"
            }
    
    async def _stage_extract(self, content: str, images: Optional[List[str]]) -> Dict[str, Any]:
        """
        é˜¶æ®µ1: æå–
        ä»åŸå§‹å†…å®¹å’Œå›¾ç‰‡ä¸­æå–åŸºç¡€ä¿¡æ¯
        """
        prompt = """ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯æå–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»ç»™å®šå†…å®¹ä¸­æå–æ‰€æœ‰å…³é”®ä¿¡æ¯ã€‚

è¯·ä»”ç»†åˆ†æå†…å®¹ï¼Œæå–ä»¥ä¸‹ä¿¡æ¯ï¼ˆJSONæ ¼å¼ï¼‰ï¼š

{
    "title": "å†…å®¹çš„æ ¸å¿ƒä¸»é¢˜/æ ‡é¢˜",
    "raw_summary": "å†…å®¹çš„åŸå§‹æ‘˜è¦ï¼ˆä¿æŒå®¢è§‚ï¼Œä¸æ·»åŠ è§£é‡Šï¼‰",
    "detected_urls": ["ä»å†…å®¹ä¸­å‘ç°çš„æ‰€æœ‰URLé“¾æ¥"],
    "detected_names": ["å‘ç°çš„é¡¹ç›®å/äº§å“å/æŠ€æœ¯å"],
    "detected_versions": ["å‘ç°çš„ç‰ˆæœ¬å·"],
    "detected_commands": ["å‘ç°çš„å‘½ä»¤è¡Œ/ä»£ç ç‰‡æ®µ"],
    "detected_features": ["å‘ç°çš„åŠŸèƒ½ç‰¹æ€§åˆ—è¡¨"],
    "content_language": "å†…å®¹è¯­è¨€ï¼ˆä¸­æ–‡/è‹±æ–‡/æ··åˆï¼‰",
    "has_code": true/false,
    "has_diagram": true/false,
    "source_hints": ["å¯èƒ½çš„æ¥æºæç¤ºï¼Œå¦‚GitHub/æ–‡æ¡£/æ•™ç¨‹ç­‰"]
}

æå–è¦æ±‚ï¼š
1. ä¿æŒä¿¡æ¯çš„åŸå§‹æ€§ï¼Œä¸è¦æ·»åŠ ä½ çš„ç†è§£
2. URLè¦å®Œæ•´æå–ï¼ŒåŒ…æ‹¬GitHubé“¾æ¥ã€æ–‡æ¡£é“¾æ¥ç­‰
3. å¦‚æœæ˜¯å¼€æºé¡¹ç›®ï¼Œç‰¹åˆ«æ³¨æ„æå–ä»“åº“åœ°å€
4. æå–æ‰€æœ‰ç‰ˆæœ¬å·ã€æ—¥æœŸç­‰å…³é”®æ•°æ®
5. ä»£ç ç‰‡æ®µåŸæ ·æå–"""

        # å¤„ç†å›¾ç‰‡
        actual_content = content
        if images and len(images) > 0:
            image_text = await ai_service.analyze_image(
                images, 
                context="è¯·è¯¦ç»†æè¿°å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—ã€ä»£ç ã€é“¾æ¥ã€å›¾è¡¨å†…å®¹ã€‚å¦‚æœæ˜¯GitHubé¡µé¢ï¼Œè¯·æå–ä»“åº“åã€staræ•°ã€æè¿°ç­‰æ‰€æœ‰å¯è§ä¿¡æ¯ã€‚"
            )
            if image_text:
                actual_content = f"[å›¾ç‰‡å†…å®¹]\n{image_text}\n\n[æ–‡å­—å†…å®¹]\n{content}" if content else f"[å›¾ç‰‡å†…å®¹]\n{image_text}"
        
        if not actual_content or len(actual_content.strip()) < 10:
            return {"error": "æ²¡æœ‰æœ‰æ•ˆå†…å®¹å¯æå–"}
        
        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"è¯·æå–ä»¥ä¸‹å†…å®¹çš„ä¿¡æ¯ï¼š\n\n{actual_content}"}
        ]
        
        result = await ai_service._call_api(messages, temperature=0.2)
        
        try:
            return self._parse_json(result)
        except:
            return {
                "title": actual_content[:100],
                "raw_summary": actual_content[:500],
                "detected_urls": self._extract_urls(actual_content),
                "detected_names": [],
                "content_language": "æœªçŸ¥"
            }
    
    async def _stage_analyze(self, extraction: Dict[str, Any]) -> Dict[str, Any]:
        """
        é˜¶æ®µ2: åˆ†æ
        æ·±åº¦ç†è§£å†…å®¹çš„ç»“æ„ã€æŠ€æœ¯ç»†èŠ‚å’Œåº”ç”¨åœºæ™¯
        """
        prompt = """ä½ æ˜¯ä¸€ä¸ªèµ„æ·±æŠ€æœ¯æ¶æ„å¸ˆå’Œæ•°æ®åˆ†æå¸ˆã€‚åŸºäºæå–çš„ä¿¡æ¯ï¼Œè¿›è¡Œæ·±åº¦åˆ†æã€‚

è¯·åˆ†æå¹¶è¾“å‡ºï¼ˆJSONæ ¼å¼ï¼‰ï¼š

{
    "content_type": "å†…å®¹ç±»å‹ï¼ˆå¼€æºé¡¹ç›®/æŠ€æœ¯æ•™ç¨‹/å·¥å…·ä»‹ç»/æ¦‚å¿µè§£é‡Š/æ–°é—»èµ„è®¯/å…¶ä»–ï¼‰",
    "domain": "æ‰€å±é¢†åŸŸï¼ˆWebå¼€å‘/åŒºå—é“¾/AI/DevOps/æ•°æ®åº“/å…¶ä»–ï¼‰",
    "tech_stack": ["æ¶‰åŠçš„æŠ€æœ¯æ ˆ"],
    "architecture_pattern": "æ¶æ„æ¨¡å¼ï¼ˆå¦‚æœ‰ï¼‰",
    "complexity_level": "å¤æ‚åº¦ï¼ˆå…¥é—¨/ä¸­çº§/é«˜çº§/ä¸“å®¶ï¼‰",
    "target_audience": "ç›®æ ‡å—ä¼—",
    "prerequisites": ["å‰ç½®çŸ¥è¯†è¦æ±‚"],
    "use_cases": ["é€‚ç”¨åœºæ™¯"],
    "advantages": ["ä¼˜ç‚¹/äº®ç‚¹"],
    "limitations": ["å±€é™æ€§/æ³¨æ„äº‹é¡¹"],
    "related_technologies": ["ç›¸å…³æŠ€æœ¯/ç«å“"],
    "learning_path": "å»ºè®®å­¦ä¹ è·¯å¾„",
    "estimated_learning_time": "é¢„ä¼°å­¦ä¹ æ—¶é—´"
}

åˆ†æè¦æ±‚ï¼š
1. ç«™åœ¨æŠ€æœ¯æ¶æ„å¸ˆè§’åº¦ï¼Œåˆ†ææŠ€æœ¯æ·±åº¦
2. è¯†åˆ«æ ¸å¿ƒä»·å€¼å’Œå·®å¼‚åŒ–ç‰¹ç‚¹
3. åˆ†æé€‚ç”¨åœºæ™¯å’Œå±€é™æ€§
4. æä¾›å®ç”¨çš„å­¦ä¹ å»ºè®®"""

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"è¯·åˆ†æä»¥ä¸‹æå–çš„ä¿¡æ¯ï¼š\n\n{json.dumps(extraction, ensure_ascii=False, indent=2)}"}
        ]
        
        result = await ai_service._call_api(messages, temperature=0.3)
        
        try:
            return self._parse_json(result)
        except:
            return {
                "content_type": "æœªçŸ¥",
                "domain": "æœªçŸ¥",
                "tech_stack": [],
                "complexity_level": "ä¸­çº§"
            }
    
    async def _stage_search(self, extraction: Dict[str, Any], analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        é˜¶æ®µ3: æœç´¢è¡¥å……
        åŸºäºæå–çš„ä¿¡æ¯ï¼Œæœç´¢è¡¥å……ä¸Šä¸‹æ–‡
        """
        prompt = """ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æœç´¢ä¸“å®¶ã€‚åŸºäºå·²çŸ¥ä¿¡æ¯ï¼Œæ¨æ–­å’Œè¡¥å……å¯èƒ½çš„ç›¸å…³ä¿¡æ¯ã€‚

ä½ éœ€è¦ï¼š
1. å¦‚æœå‘ç°äº†é¡¹ç›®åä½†æ²¡æœ‰URLï¼Œæ¨æ–­å¯èƒ½çš„GitHub/å®˜ç½‘åœ°å€
2. è¡¥å……å¸¸è§çš„å®‰è£…æ–¹å¼å’Œä½¿ç”¨å‘½ä»¤
3. æ¨æ–­ç›¸å…³çš„æ–‡æ¡£ã€æ•™ç¨‹èµ„æº
4. è¯†åˆ«ç›¸å…³çš„ç”Ÿæ€ç³»ç»Ÿå·¥å…·

è¯·è¾“å‡ºï¼ˆJSONæ ¼å¼ï¼‰ï¼š

{
    "inferred_github_url": "æ¨æ–­çš„GitHubåœ°å€ï¼ˆå¦‚æœæ˜¯å¼€æºé¡¹ç›®ï¼‰",
    "inferred_docs_url": "æ¨æ–­çš„æ–‡æ¡£åœ°å€",
    "inferred_website": "æ¨æ–­çš„å®˜ç½‘åœ°å€",
    "found_urls": ["æ‰€æœ‰å·²çŸ¥å’Œæ¨æ–­çš„URLåˆ—è¡¨"],
    "install_commands": {
        "npm": "npm install xxx",
        "pip": "pip install xxx",
        "docker": "docker pull xxx",
        "other": "å…¶ä»–å®‰è£…æ–¹å¼"
    },
    "quick_start": "å¿«é€Ÿå¼€å§‹æ­¥éª¤",
    "related_resources": [
        {"name": "èµ„æºå", "url": "èµ„æºURL", "type": "æ–‡æ¡£/æ•™ç¨‹/è§†é¢‘"}
    ],
    "ecosystem_tools": ["ç”Ÿæ€ç³»ç»Ÿç›¸å…³å·¥å…·"],
    "community": {
        "discord": "Discordé“¾æ¥",
        "twitter": "Twitteré“¾æ¥",
        "forum": "è®ºå›é“¾æ¥"
    }
}

æ³¨æ„ï¼š
1. åªæ¨æ–­ä½ æœ‰æŠŠæ¡çš„ä¿¡æ¯
2. GitHubé¡¹ç›®é€šå¸¸æ ¼å¼ä¸º https://github.com/owner/repo
3. npmåŒ…é€šå¸¸æœ‰ npmjs.com é¡µé¢
4. ä¸»æµé¡¹ç›®é€šå¸¸æœ‰å®˜æ–¹æ–‡æ¡£ç«™ç‚¹"""

        context = {
            "title": extraction.get("title"),
            "names": extraction.get("detected_names", []),
            "urls": extraction.get("detected_urls", []),
            "tech_stack": analysis.get("tech_stack", []),
            "domain": analysis.get("domain")
        }
        
        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"åŸºäºä»¥ä¸‹ä¿¡æ¯è¿›è¡Œæœç´¢è¡¥å……ï¼š\n\n{json.dumps(context, ensure_ascii=False, indent=2)}"}
        ]
        
        result = await ai_service._call_api(messages, temperature=0.4)
        
        try:
            return self._parse_json(result)
        except:
            return {
                "found_urls": extraction.get("detected_urls", []),
                "install_commands": {},
                "quick_start": ""
            }
    
    async def _stage_verify(
        self, 
        extraction: Dict[str, Any], 
        analysis: Dict[str, Any],
        enriched: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        é˜¶æ®µ4: éªŒè¯
        äº¤å‰éªŒè¯å…³é”®ä¿¡æ¯çš„å‡†ç¡®æ€§
        """
        prompt = """ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ä¿¡æ¯éªŒè¯ä¸“å®¶ã€‚è¯·éªŒè¯ä»¥ä¸‹ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚

éªŒè¯è¦ç‚¹ï¼š
1. URLæ ¼å¼æ˜¯å¦æ­£ç¡®
2. æŠ€æœ¯æ ˆä¿¡æ¯æ˜¯å¦åŒ¹é…
3. ç‰ˆæœ¬å·æ˜¯å¦åˆç†
4. å‘½ä»¤è¯­æ³•æ˜¯å¦æ­£ç¡®
5. ä¿¡æ¯æ˜¯å¦è‡ªæ´½

è¯·è¾“å‡ºï¼ˆJSONæ ¼å¼ï¼‰ï¼š

{
    "confidence": 0.0-1.0,
    "verified_items": [
        {"item": "xxx", "status": "verified/unverified/uncertain", "note": "å¤‡æ³¨"}
    ],
    "corrections": [
        {"original": "åŸå§‹ä¿¡æ¯", "corrected": "ä¿®æ­£å", "reason": "ä¿®æ­£åŸå› "}
    ],
    "warnings": ["éœ€è¦æ³¨æ„çš„é—®é¢˜"],
    "missing_critical_info": ["ç¼ºå¤±çš„å…³é”®ä¿¡æ¯"],
    "data_quality_score": 0-100,
    "recommendation": "ä¿¡æ¯è´¨é‡è¯„ä»·å’Œå»ºè®®"
}

éªŒè¯æ ‡å‡†ï¼š
- verified: ä¿¡æ¯ç¡®å®šæ­£ç¡®
- unverified: æ— æ³•éªŒè¯
- uncertain: ä¿¡æ¯å¯èƒ½æœ‰è¯¯"""

        all_info = {
            "extraction": extraction,
            "analysis": analysis,
            "enriched": enriched
        }
        
        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"è¯·éªŒè¯ä»¥ä¸‹ä¿¡æ¯ï¼š\n\n{json.dumps(all_info, ensure_ascii=False, indent=2)}"}
        ]
        
        result = await ai_service._call_api(messages, temperature=0.2)
        
        try:
            return self._parse_json(result)
        except:
            return {
                "confidence": 0.5,
                "verified_items": [],
                "corrections": [],
                "warnings": ["éªŒè¯è¿‡ç¨‹å‡ºç°å¼‚å¸¸"]
            }
    
    async def _stage_synthesize(
        self,
        extraction: Dict[str, Any],
        analysis: Dict[str, Any],
        enriched: Dict[str, Any],
        verification: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        é˜¶æ®µ5: å½’çº³æ€»ç»“
        ç»¼åˆæ‰€æœ‰ä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç»ˆçš„çŸ¥è¯†å¡ç‰‡
        """
        prompt = """ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†ç®¡ç†ä¸“å®¶ã€‚è¯·ç»¼åˆæ‰€æœ‰é˜¶æ®µçš„åˆ†æç»“æœï¼Œç”Ÿæˆä¸€ä¸ªå…¨é¢ã€å‡†ç¡®ã€å®ç”¨çš„çŸ¥è¯†å¡ç‰‡ã€‚

çŸ¥è¯†å¡ç‰‡è¦æ±‚ï¼š
1. æ ‡é¢˜ï¼šç®€æ´æ˜äº†ï¼Œçªå‡ºæ ¸å¿ƒä»·å€¼
2. æ‘˜è¦ï¼š200å­—å·¦å³ï¼Œæ¶µç›–æ˜¯ä»€ä¹ˆã€èƒ½åšä»€ä¹ˆã€ä¸ºä»€ä¹ˆé‡è¦
3. å…³é”®ç‚¹ï¼š5-8ä¸ªï¼Œå…·ä½“å¯æ“ä½œ
4. æ ‡ç­¾ï¼šç²¾å‡†çš„æŠ€æœ¯æ ‡ç­¾ï¼Œä¾¿äºæœç´¢
5. ä½¿ç”¨ç¤ºä¾‹ï¼šå®é™…å¯è¿è¡Œçš„ä»£ç /å‘½ä»¤
6. éƒ¨ç½²æŒ‡å—ï¼šå¦‚æœæ˜¯é¡¹ç›®ï¼Œæä¾›éƒ¨ç½²æ­¥éª¤

è¯·è¾“å‡ºï¼ˆJSONæ ¼å¼ï¼‰ï¼š

{
    "title": "çŸ¥è¯†æ ‡é¢˜",
    "summary": "å…¨é¢çš„æ‘˜è¦ï¼ˆåŒ…å«æ ¸å¿ƒä»·å€¼ã€é€‚ç”¨åœºæ™¯ã€æŠ€æœ¯ç‰¹ç‚¹ï¼‰",
    "key_points": [
        "å…³é”®ç‚¹1ï¼ˆå…·ä½“ã€å¯æ“ä½œï¼‰",
        "å…³é”®ç‚¹2",
        "å…³é”®ç‚¹3"
    ],
    "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"],
    "category": "åˆ†ç±»ï¼ˆæŠ€æœ¯/å·¥å…·/æ¦‚å¿µ/æ•™ç¨‹ï¼‰",
    "difficulty": "éš¾åº¦ï¼ˆå…¥é—¨/ä¸­çº§/é«˜çº§ï¼‰",
    "action_items": [
        "å¯æ‰§è¡Œçš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨1",
        "è¡ŒåŠ¨2"
    ],
    "usage_example": "```language\\nå®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹ä»£ç \\n```",
    "deployment_guide": "éƒ¨ç½²æ­¥éª¤ï¼ˆå¦‚é€‚ç”¨ï¼‰ï¼š\\n1. æ­¥éª¤1\\n2. æ­¥éª¤2",
    "is_open_source": true/false,
    "repo_url": "GitHubä»“åº“åœ°å€ï¼ˆå¦‚æœ‰ï¼‰",
    "official_docs": "å®˜æ–¹æ–‡æ¡£åœ°å€",
    "quick_reference": {
        "install": "å®‰è£…å‘½ä»¤",
        "run": "è¿è¡Œå‘½ä»¤",
        "docs": "æ–‡æ¡£é“¾æ¥"
    },
    "related_topics": ["ç›¸å…³ä¸»é¢˜1", "ç›¸å…³ä¸»é¢˜2"],
    "learning_resources": [
        {"name": "èµ„æºå", "url": "é“¾æ¥", "type": "ç±»å‹"}
    ],
    "pros_cons": {
        "pros": ["ä¼˜ç‚¹1", "ä¼˜ç‚¹2"],
        "cons": ["å±€é™1", "å±€é™2"]
    },
    "best_practices": ["æœ€ä½³å®è·µ1", "æœ€ä½³å®è·µ2"],
    "common_mistakes": ["å¸¸è§é”™è¯¯1", "å¸¸è§é”™è¯¯2"]
}

ç”Ÿæˆè¦æ±‚ï¼š
1. ä¿¡æ¯å¿…é¡»åŸºäºå‰é¢é˜¶æ®µçš„åˆ†æï¼Œä¸è¦ç¼–é€ 
2. ä½¿ç”¨ç¤ºä¾‹å¿…é¡»æ˜¯å¯è¿è¡Œçš„
3. å¦‚æœæ˜¯GitHubé¡¹ç›®ï¼Œå¿…é¡»åŒ…å«ä»“åº“åœ°å€
4. æ ‡ç­¾è¦ç²¾å‡†ï¼Œä¾¿äºåç»­æœç´¢
5. è€ƒè™‘å®é™…ä½¿ç”¨åœºæ™¯ï¼Œæä¾›å®ç”¨å»ºè®®"""

        all_info = {
            "extraction": extraction,
            "analysis": analysis,
            "enriched": enriched,
            "verification": verification
        }
        
        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"è¯·åŸºäºä»¥ä¸‹æ‰€æœ‰é˜¶æ®µçš„åˆ†æç»“æœï¼Œç”ŸæˆçŸ¥è¯†å¡ç‰‡ï¼š\n\n{json.dumps(all_info, ensure_ascii=False, indent=2)}"}
        ]
        
        result = await ai_service._call_api(messages, temperature=0.4)
        
        try:
            parsed = self._parse_json(result)
            
            # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
            return {
                "title": parsed.get("title", extraction.get("title", "æœªçŸ¥")),
                "summary": parsed.get("summary", extraction.get("raw_summary", "")),
                "key_points": parsed.get("key_points", []),
                "tags": parsed.get("tags", []),
                "category": parsed.get("category", analysis.get("domain", "æœªåˆ†ç±»")),
                "difficulty": parsed.get("difficulty", analysis.get("complexity_level", "ä¸­çº§")),
                "action_items": parsed.get("action_items", []),
                "usage_example": parsed.get("usage_example"),
                "deployment_guide": parsed.get("deployment_guide"),
                "is_open_source": parsed.get("is_open_source", False),
                "repo_url": parsed.get("repo_url") or enriched.get("inferred_github_url"),
                "official_docs": parsed.get("official_docs"),
                "quick_reference": parsed.get("quick_reference"),
                "related_topics": parsed.get("related_topics", []),
                "learning_resources": parsed.get("learning_resources", []),
                "pros_cons": parsed.get("pros_cons"),
                "best_practices": parsed.get("best_practices", []),
                "common_mistakes": parsed.get("common_mistakes", [])
            }
        except Exception as e:
            logger.error(f"Synthesize parsing error: {e}")
            return {
                "title": extraction.get("title", "å¤„ç†å¤±è´¥"),
                "summary": extraction.get("raw_summary", ""),
                "key_points": [],
                "tags": analysis.get("tech_stack", []),
                "category": analysis.get("domain", "æœªåˆ†ç±»"),
                "difficulty": analysis.get("complexity_level", "ä¸­çº§"),
                "action_items": []
            }
    
    def _parse_json(self, text: str) -> Dict[str, Any]:
        """è§£æ JSON å“åº”"""
        if not text:
            return {}
        
        # æ¸…ç† markdown ä»£ç å—
        cleaned = text.strip()
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        if cleaned.startswith("```"):
            cleaned = cleaned[3:]
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
        
        return json.loads(cleaned.strip())
    
    def _extract_urls(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå– URL"""
        url_pattern = r'https?://[^\s<>"\')\]]*'
        urls = re.findall(url_pattern, text)
        return list(set(urls))


# å…¨å±€å®ä¾‹
distillation_pipeline = DistillationPipeline()

